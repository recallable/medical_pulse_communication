import glob
import os
from typing import List

from langchain_community.embeddings import DashScopeEmbeddings
from langchain_core.documents import Document
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_postgres import PGVector
from langchain_text_splitters import MarkdownHeaderTextSplitter
from llama_parse import LlamaParse, ResultType

from core.ai import llm

# ================= 1. é…ç½®åŒºåŸŸ =================
# API Keys
LLAMA_CLOUD_API_KEY = 'llx-aoa7Ko4Qc7VRuHooMqxWbOhRJZq3pHwNH67QlzL9gMOdYJPi'
# ç¡®ä¿ç¯å¢ƒå˜é‡ä¸­æœ‰ DASHSCOPE_API_KEYï¼Œæˆ–è€…ç›´æ¥å¡«åœ¨è¿™é‡Œ
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

# ç›®å½•é…ç½®
INPUT_DIR = "../docs"  # PDF æ‰€åœ¨çš„æ ¹ç›®å½•
MD_OUTPUT_DIR = "../docs_markdown"  # è§£æåçš„ Markdown å­˜æ”¾ç›®å½• (è‡ªåŠ¨åˆ›å»º)

# æ•°æ®åº“è¿æ¥
DB_CONNECTION = "postgresql+psycopg2://postgres:121518@localhost:5432/rag_db"
COLLECTION_NAME = "h1n1_knowledge_base"

# ================= 2. åˆå§‹åŒ–å…¨å±€ç»„ä»¶ =================
# (è¿™äº›ç»„ä»¶åªéœ€è¦åˆå§‹åŒ–ä¸€æ¬¡ï¼Œä¸éœ€è¦åœ¨å¾ªç¯é‡Œåå¤åˆ›å»º)

# åˆå§‹åŒ– LlamaParse
parser = LlamaParse(
    result_type=ResultType.MD,
    api_key=LLAMA_CLOUD_API_KEY,
    language="ch_sim",
    system_prompt="""
    ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£é‡æ„ä¸è½¬æ¢ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†PDFå†…å®¹è½¬æ¢ä¸ºè¯­ä¹‰è¿è´¯ã€ç»“æ„æ¸…æ™°çš„Markdownæ–‡æ¡£ã€‚
    è¯·ä¸¥æ ¼æ‰§è¡Œä»¥ä¸‹â€œæ¸…æ´—-é‡æ„-æ ¼å¼åŒ–â€ä¸‰æ­¥æµç¨‹ï¼š

    1. ã€ç¬¬ä¸€æ­¥ï¼šå»é™¤å™ªéŸ³ä¸ä¿®å¤åˆ†é¡µï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰ã€‘
       - **è¯†åˆ«å‡æ ‡é¢˜ï¼ˆé¡µçœ‰å™ªéŸ³ï¼‰**ï¼šæ–‡æ¡£ä¸­é‡å¤å‡ºç°çš„â€œä¸€ã€ç”²æµç®€ä»‹â€ã€â€œç”²å‹H1N1æµæ„ŸåŒ»ç–—çŸ¥è¯†åº“â€ç­‰é€šå¸¸æ˜¯é¡µçœ‰ã€‚å¦‚æœè¿™äº›æ–‡æœ¬å‡ºç°åœ¨æ®µè½ä¸­é—´ï¼Œæˆ–è€…åˆ‡æ–­äº†å­ç« èŠ‚ï¼ˆå¦‚å‡ºç°åœ¨â€œ3.1 æ ¸å¿ƒç—…å› â€å’Œå…¶æ­£æ–‡ä¹‹é—´ï¼‰ï¼Œ**å¿…é¡»å°†å…¶è§†ä¸ºå™ªéŸ³ç›´æ¥åˆ é™¤**ï¼Œä¸¥ç¦ä¿ç•™ä¸ºæ ‡é¢˜ã€‚
       - **è·¨é¡µè¯­ä¹‰åˆå¹¶**ï¼šå½“é‡åˆ°å­æ ‡é¢˜ï¼ˆå¦‚â€œ3.1 æ ¸å¿ƒç—…å› â€ï¼‰åç´§æ¥é¡µçœ‰å™ªéŸ³æ—¶ï¼Œè¯·å¿½ç•¥é¡µçœ‰ï¼Œå°†ä¸‹ä¸€é¡µçš„æ­£æ–‡ç›´æ¥æ‹¼æ¥åˆ°è¯¥å­æ ‡é¢˜ä¸‹æ–¹ã€‚ç¡®ä¿â€œ3.1â€çš„å†…å®¹ä¸ä¸ºç©ºï¼Œä¿æŒè¯­ä¹‰è¿è´¯ã€‚
       - **å»é™¤å…ƒæ•°æ®**ï¼šä¸¥ç¦è¾“å‡ºâ€œ**æ ‡é¢˜ï¼š**â€ã€â€œ**æ­£æ–‡ï¼š**â€ã€â€œâ€ç­‰æ ‡ç­¾ï¼›å»é™¤æ‰€æœ‰é¡µç ä¿¡æ¯ï¼ˆå¦‚â€œPAGE 1â€ï¼‰ã€‚

    2. ã€ç¬¬äºŒæ­¥ï¼šå»ºç«‹æ ‡é¢˜å±‚çº§ã€‘
       - ä»…å¯¹**çœŸæ­£çš„ç« èŠ‚èµ·å§‹**åº”ç”¨æ ‡é¢˜æ ¼å¼ï¼š
         - ä¸­æ–‡æ•°å­—å¼€å¤´çš„ç« èŠ‚ï¼ˆå¦‚â€œä¸€ã€ç”²æµç®€ä»‹â€ï¼‰ï¼šä½¿ç”¨ä¸€çº§æ ‡é¢˜ "# "ã€‚
         - æ•°å­—ç¼–å·çš„å°èŠ‚ï¼ˆå¦‚â€œ2.1 ç—…åŸå­¦ç‰¹å¾â€ï¼‰ï¼šä½¿ç”¨äºŒçº§æ ‡é¢˜ "## "ã€‚
         - å…¶ä»–åŠ ç²—å°æ ‡é¢˜ï¼šä½¿ç”¨ä¸‰çº§æ ‡é¢˜ "### "ã€‚

    3. ã€ç¬¬ä¸‰æ­¥ï¼šæ ¼å¼åŒ–å†…å®¹ã€‘
       - **è¡¨æ ¼**ï¼šå¿…é¡»å°†è¡¨æ ¼è½¬æ¢ä¸ºæ ‡å‡†çš„ Markdown è¡¨æ ¼è¯­æ³•ï¼Œç¡®ä¿æ•°æ®ä¸ä¸¢å¤±ã€‚
       - **æ­£æ–‡**ï¼šä¿æŒæ®µè½å®Œæ•´ï¼Œç§»é™¤å¤šä½™çš„æ¢è¡Œç¬¦ã€‚

    ç›®æ ‡ï¼šè¾“å‡ºä¸€ä»½å¯ä»¥ç›´æ¥ç”¨äºRAGæ£€ç´¢çš„çº¯å‡€Markdownï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚
    """
)

# åˆå§‹åŒ–åˆ‡åˆ†å™¨
headers_to_split_on = [
    ("#", "Chapter"),
    ("##", "Section"),
    ("###", "Subsection"),
]
markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)

# åˆå§‹åŒ– Embeddings
embeddings = DashScopeEmbeddings(model="text-embedding-v1", dashscope_api_key=DASHSCOPE_API_KEY)

# åˆå§‹åŒ– VectorStore
vector_store = PGVector(
    embeddings=embeddings,
    collection_name=COLLECTION_NAME,
    connection=DB_CONNECTION,
    use_jsonb=True,
)


# ================= 3. æ ¸å¿ƒå¤„ç†é€»è¾‘ =================

def process_directory(directory_path):
    # 1. é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .pdf æ–‡ä»¶
    # glob æ¨¡å¼ '**/*.pdf' é…åˆ recursive=True å¯ä»¥ç©¿é€å­ç›®å½•
    pdf_files = glob.glob(os.path.join(directory_path, "**/*.pdf"), recursive=True)

    print(f"ğŸ“‚ æ‰«æç›®å½•: {directory_path}")
    print(f"ğŸ“„ å‘ç° PDF æ–‡ä»¶æ•°: {len(pdf_files)}")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(MD_OUTPUT_DIR):
        os.makedirs(MD_OUTPUT_DIR)

    # 2. å¾ªç¯å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for index, pdf_path in enumerate(pdf_files):
        try:
            filename = os.path.basename(pdf_path)
            print(f"\n[{index + 1}/{len(pdf_files)}] ğŸš€ æ­£åœ¨å¤„ç†: {filename}")

            # --- A. è§£æ PDF (LlamaParse) ---
            documents = parser.load_data(pdf_path)
            if not documents:
                print(f"âš ï¸ è·³è¿‡: {filename} è§£æç»“æœä¸ºç©º")
                continue

            raw_markdown = "\n\n".join([doc.text for doc in documents])

            # --- B. ä¿å­˜ Markdown å¤‡ä»½ (å¯é€‰) ---
            # ä¿æŒæ–‡ä»¶åä¸€è‡´ï¼Œåªæ”¹åç¼€
            md_filename = f"{os.path.splitext(filename)[0]}.md"
            md_save_path = os.path.join(MD_OUTPUT_DIR, md_filename)

            with open(md_save_path, "w", encoding="utf-8") as f:
                f.write(raw_markdown)
            print(f"   ğŸ’¾ Markdown å·²ä¿å­˜è‡³: {md_save_path}")

            # --- C. åˆ‡åˆ†æ–‡æœ¬ ---
            splits = markdown_splitter.split_text(raw_markdown)

            # --- D. æ³¨å…¥å…ƒæ•°æ® (å…³é”®æ­¥éª¤ï¼) ---
            # è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼šæˆ‘ä»¬éœ€è¦ç»™æ¯ä¸ªåˆ‡ç‰‡æ‰“ä¸Šæ ‡ç­¾ï¼ŒçŸ¥é“å®ƒæ¥è‡ªå“ªä¸ªæ–‡ä»¶
            for split in splits:
                # ä¿ç•™åŸæœ‰çš„æ ‡é¢˜å…ƒæ•°æ®ï¼Œå¹¶å¢åŠ æ¥æºä¿¡æ¯
                split.metadata["source"] = filename
                split.metadata["file_path"] = pdf_path

            print(f"   âœ‚ï¸ åˆ‡åˆ†å¾—åˆ° {len(splits)} ä¸ªç‰‡æ®µ")

            # --- E. å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“ ---
            # æ‰¹é‡æ’å…¥å½“å‰æ–‡ä»¶çš„æ‰€æœ‰åˆ‡ç‰‡
            vector_store.add_documents(splits)
            print(f"   âœ… {filename} å…¥åº“æˆåŠŸï¼")

        except Exception as e:
            # æ•è·å¼‚å¸¸ï¼Œé˜²æ­¢ä¸€ä¸ªæ–‡ä»¶æŠ¥é”™å¯¼è‡´æ•´ä¸ªç¨‹åºåœæ­¢
            print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue


def process_single_markdown(file_path):
    """
    è¯»å–å•ä¸ª Markdown æ–‡ä»¶å¹¶å­˜å…¥å‘é‡æ•°æ®åº“
    """
    # 0. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ -> {file_path}")
        return

    try:
        filename = os.path.basename(file_path)
        print(f"\nğŸš€ æ­£åœ¨å¤„ç†å•æ–‡ä»¶: {filename}")

        # 1. è¯»å– Markdown æ–‡ä»¶å†…å®¹
        with open(file_path, "r", encoding="utf-8") as f:
            raw_markdown = f.read()

        print(f"   ğŸ“– è¯»å–æˆåŠŸï¼Œå­—ç¬¦æ•°: {len(raw_markdown)}")

        # 2. åˆ‡åˆ†æ–‡æœ¬ (å¤ç”¨å…¨å±€å®šä¹‰çš„ markdown_splitter)
        splits = markdown_splitter.split_text(raw_markdown)

        # 3. æ³¨å…¥å…ƒæ•°æ® (å…³é”®æ­¥éª¤)
        for split in splits:
            # è®°å½•æ¥æºï¼Œæ–¹ä¾¿åç»­æ£€ç´¢æ—¶æº¯æº
            split.metadata["source"] = filename
            split.metadata["file_path"] = file_path

        print(f"   âœ‚ï¸ åˆ‡åˆ†å¾—åˆ° {len(splits)} ä¸ªç‰‡æ®µ")

        # 4. å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“ (å¤ç”¨å…¨å±€å®šä¹‰çš„ vector_store)
        if splits:
            vector_store.add_documents(splits)
            print(f"   âœ… {filename} å…¥åº“æˆåŠŸï¼")
        else:
            print(f"   âš ï¸ è­¦å‘Š: æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æœªèƒ½åˆ‡åˆ†å‡ºä»»ä½•ç‰‡æ®µ")

    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‘ç”Ÿé”™è¯¯: {e}")


def chat(question: str):
    """
    AI èŠå¤©æ¥å£ (èåˆä¸‰ç§ RAG ç­–ç•¥)
    """

    # ==================================================
    # 3. ã€ç­–ç•¥äºŒ & ä¸‰ï¼šå¤šè·¯æ‰©å±•ä¸åˆ†è§£ã€‘ (Expansion & Decomposition)
    # ç›®çš„ï¼šç”Ÿæˆå¤šä¸ªæœç´¢è§†è§’å’Œå­é—®é¢˜
    # ==================================================
    # è¿™ä¸€æ­¥ä¼šç”Ÿæˆä¸€ä¸ªåˆ—è¡¨ï¼Œä¾‹å¦‚ ["ç”²æµæ²»ç–—æ–¹æ¡ˆ", "å„¿ç«¥ç”²æµç”¨è¯", "ç”²æµå‘çƒ§æŠ¤ç†"]
    queries_to_search = generate_multi_queries(question)
    print(f"ğŸš€ [ç­–ç•¥2&3] ç”Ÿæˆçš„æœç´¢è¯: {queries_to_search}")

    # ==================================================
    # 4. ã€å¹¶è¡Œå‘é‡æ£€ç´¢ & å»é‡ã€‘ (Retrieval & Deduplication)
    # ç›®çš„ï¼šæ‹¿ç€æ‰€æœ‰æœç´¢è¯å»åº“é‡Œæ‰¾ï¼Œå¹¶åˆå¹¶ç»“æœ
    # ==================================================
    all_docs = []

    # éå†æ‰€æœ‰ç”Ÿæˆçš„æŸ¥è¯¢è¯è¿›è¡Œæ£€ç´¢
    # æ³¨æ„ï¼švector_store.similarity_search æ˜¯åŒæ­¥çš„ï¼Œè¿™é‡Œç”¨å¾ªç¯
    for q in queries_to_search:
        # è¿™é‡Œçš„ k=2 å¯ä»¥å°ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬æœäº†å¾ˆå¤šæ¬¡ï¼Œæ€»é‡ä¼šå¾ˆå¤š
        docs = vector_store.similarity_search(q, k=2)
        all_docs.extend(docs)

    # ã€æ–‡æ¡£å»é‡ã€‘ï¼šæ ¹æ® page_content å»é‡ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡é‡å¤
    unique_docs = deduplicate_documents(all_docs)
    print(f"ğŸ“š [æœ€ç»ˆ] æ£€ç´¢åˆ° {len(unique_docs)} ä¸ªä¸é‡å¤ç‰‡æ®µ")

    # æ„å»ºä¸Šä¸‹æ–‡
    context_text = "\n\n".join([doc.page_content for doc in unique_docs])
    messages = [
        SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªRagè¯„ä¼°ä¸“å®¶, è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’ŒRAGæ£€ç´¢ç»“æœ,è¯„ä¼°ä¸€ä¸‹å¬å›ç‡ã€‚'),
        HumanMessage(content=f'ç”¨æˆ·çš„é—®é¢˜: {question}, RAGæ£€ç´¢ç»“æœ: {context_text}')
    ]
    response = llm.invoke(messages)
    print(response.content)
    # ==================================================
    # 5. ç”Ÿæˆæœ€ç»ˆå›ç­”
    # ==================================================
    rag_system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ã€å‚è€ƒä¿¡æ¯ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

        å›ç­”åŸåˆ™ï¼š
        1. ç»¼åˆå¤šæ¡å‚è€ƒä¿¡æ¯ï¼Œé€»è¾‘æ¸…æ™°åœ°å›ç­”ã€‚
        2. å¦‚æœå‚è€ƒä¿¡æ¯ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ï¼Œä¸è¦çç¼–ã€‚
        3. è¯­æ°”äº²åˆ‡ã€ä¸“ä¸šã€‚

        ã€å‚è€ƒä¿¡æ¯ã€‘:
        {context_text}
        """

    final_messages = [
        SystemMessage(content=rag_system_prompt),
        HumanMessage(content=question)  # ç»™ LLM çœ‹åŸå§‹é—®é¢˜ï¼Œä¿æŒå¯¹è¯æµç•…åº¦
    ]

    response = llm.invoke(final_messages)

    return response.content


# -------------------------------------------------------------------------
# è¾…åŠ©æ–¹æ³•åŒºåŸŸ
# -------------------------------------------------------------------------

def generate_multi_queries(original_query: str) -> List[str]:
    """
    ã€ç­–ç•¥äºŒ & ä¸‰å®ç°ã€‘ï¼šå¤šè§’åº¦æ‰©å±• + é—®é¢˜åˆ†è§£
    """
    prompt = """
        ä½ æ˜¯ä¸€ä¸ªAIæœç´¢åŠ©æ‰‹ã€‚ä¸ºäº†æ›´ç²¾å‡†åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·åŸºäºåŸå§‹é—®é¢˜ç”Ÿæˆ 3 ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢è¯ã€‚

        ç”Ÿæˆè§„åˆ™ï¼š
        1. **åŒä¹‰æ‰©å±•**ï¼šåŒ…å«ç›¸å…³çš„åŒ»å­¦æœ¯è¯­æˆ–åˆ«åï¼ˆå¦‚"å‘çƒ§"->"å‘çƒ­å¤„ç†"ï¼‰ã€‚
        2. **é—®é¢˜æ‹†è§£**ï¼šå¦‚æœé—®é¢˜å¤æ‚ï¼Œæ‹†è§£ä¸ºå­é—®é¢˜ï¼ˆå¦‚"ç”²æµä¹™æµåŒºåˆ«"->"ç”²æµç—‡çŠ¶"å’Œ"ä¹™æµç—‡çŠ¶"ï¼‰ã€‚
        3. **ä¿ç•™åŸæ„**ï¼šå¿…é¡»åŒ…å«åŸå§‹é—®é¢˜çš„æ ¸å¿ƒæŸ¥è¯¢ã€‚

        è¯·ç›´æ¥è¾“å‡º 3 è¡ŒæŸ¥è¯¢è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦å¸¦åºå·æˆ–è§£é‡Šã€‚

        åŸå§‹é—®é¢˜: {question}

        ç¦æ­¢ï¼š
            ç¦æ­¢ä¿®æ”¹ç”¨æˆ·åŸæœ¬çš„æ„æ€ï¼Œæ¯”å¦‚ï¼šæˆ‘å˜´é‡Œé¢æœ‰ç‚¹ç–¼ï¼Œæ€ä¹ˆå›äº‹ï¼Ÿç¦æ­¢ä¿®æ”¹æˆï¼šç‰™ç–¼éƒ½æœ‰å“ªäº›ç—‡çŠ¶ï¼Ÿ
        """

    messages = [SystemMessage(content=prompt.format(question=original_query))]

    response = llm.invoke(messages)
    content = response.content.strip()

    # è§£æç»“æœï¼ŒæŒ‰è¡Œåˆ†å‰²
    queries = [q.strip() for q in content.split('\n') if q.strip()]

    # å…œåº•ï¼šå¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè‡³å°‘ä¿ç•™åŸé—®é¢˜
    if not queries:
        return [original_query]

    # æŠŠåŸå§‹é—®é¢˜ä¹ŸåŠ è¿›å»ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
    if original_query not in queries:
        queries.insert(0, original_query)

    return queries[:4]  # é™åˆ¶æœ€å¤šæœ 4 æ¬¡ï¼Œé˜²æ­¢å¤ªæ…¢


def deduplicate_documents(documents: List[Document]) -> List[Document]:
    """
    æ–‡æ¡£å»é‡å·¥å…·ï¼šæ ¹æ® page_content å»é‡
    """
    unique_docs = []
    seen_content = set()

    for doc in documents:
        # å–å†…å®¹çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹ï¼Œæˆ–è€…ç›´æ¥ç”¨æ•´ä¸ªcontent
        content_fingerprint = doc.page_content.strip()

        if content_fingerprint not in seen_content:
            seen_content.add(content_fingerprint)
            unique_docs.append(doc)

    return unique_docs


# ================= 4. æ‰§è¡Œ =================
if __name__ == "__main__":
    # process_directory(INPUT_DIR)
    # single_md_file = "../docs_markdown/å„¿ç«¥å‘çƒ­å®¶åº­æŠ¤ç†æ‰‹å†Œ.md"

    # docs_markdown_list = [
    #     '../docs_markdown/ä¸Šå‘¼å¸é“æ„ŸæŸ“åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/ä¸‹å‘¼å¸é“æ„ŸæŸ“åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/ä»£è°¢ç»¼åˆå¾åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/å…³èŠ‚ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/å†…åˆ†æ³Œä¸ä»£è°¢æ€§ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/å† å¿ƒç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/å°¿è·¯ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/å¸¸è§ä¼ æŸ“ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/å¿ƒåŠ›è¡°ç«­åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/å¿ƒå¾‹å¤±å¸¸åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/å¿ƒè¡€ç®¡ç³»ç»Ÿç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/æ…¢æ€§å‘¼å¸ç³»ç»Ÿç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/æ³Œå°¿ç³»ç»Ÿç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/æ¶ˆåŒ–ç³»ç»Ÿç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/ç”²çŠ¶è…ºç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/ç¥ç»ç³»ç»Ÿç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/ç¥ç»é€€è¡Œæ€§ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/ç²¾ç¥å¿ƒç†ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/ç³–å°¿ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/è‚èƒ†èƒ°ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/è‚¾è„ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/èƒƒè‚ é“ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/è„‘è¡€ç®¡ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/éª¨éª¼ç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/éª¨éª¼è‚Œè‚‰ç³»ç»Ÿç–¾ç—…åŒ»ç–—çŸ¥è¯†åº“.md',
    #     '../docs_markdown/é«˜è¡€å‹åŒ»ç–—çŸ¥è¯†åº“.md',
    # ]
    # for idx in trange(len(docs_markdown_list)):
    #     process_single_markdown(docs_markdown_list[idx])
    # process_single_markdown(single_md_file)
    query = 'æ€ä¹ˆåˆ¤æ–­è‡ªå·±æœ‰æ²¡æœ‰ç²¾ç¥ç—…?'
    chat(query)
    # results = vector_store.similarity_search(query, k=3)
    # for i, doc in enumerate(results):
    #     print(f"   --- ç»“æœ {i + 1} ---")
    #     print(f"   [æ¥æº]: {doc.metadata.get('source', 'æœªçŸ¥')}")
    #     print(f"   [å†…å®¹]: {doc.page_content}")  # åªæ‰“å°å‰100å­—é¢„è§ˆ
    #
    # llm = init_chat_model(
    #     model='qwen-flash',
    #     model_provider='openai',
    #     api_key=os.getenv('OPENAI_API_KEY'),
    # )
    # response = llm.invoke(f"è¯„ä¼°ä¸€ä¸‹å¬å›ç‡: ç”¨æˆ·çš„é—®é¢˜:{query}, RAGæ£€ç´¢çš„æ•°æ®:{results}")
    # print(response.content)
    # print("\nğŸ‰ æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæ¯•ï¼")
