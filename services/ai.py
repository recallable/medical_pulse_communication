import json
from typing import List

from langchain_core.documents import Document
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

from core.ai import llm, vector_store


class AiService:
    async def chat(self, user_id: int, question: str, session_id: str):
        """
        AI èŠå¤©æ¥å£ (èåˆä¸‰ç§ RAG ç­–ç•¥)
        """
        chat_message_list_key = f'chat:message:list:{user_id}:{session_id}'
        from core.redis_client import redis_client_manager as redis
        redis_client = redis.get_client()

        # ==================================================
        # 1. è·å–å¹¶æ„å»ºå†å²è®°å½•
        # ==================================================
        history_json_list = await redis_client.lrange(chat_message_list_key, -10, -1) or []
        history_messages = []
        for item in history_json_list:
            msg = json.loads(item)
            if msg['role'] == 'user':
                history_messages.append(HumanMessage(content=msg['content']))
            elif msg['role'] == 'assistant':
                history_messages.append(AIMessage(content=msg['content']))

        # ==================================================
        # 2. ã€ç­–ç•¥ä¸€ï¼šå†å²ä¸Šä¸‹æ–‡é‡å†™ã€‘ (History Awareness)
        # ç›®çš„ï¼šå¤„ç†æŒ‡ä»£æ¶ˆè§£ (å¦‚ "å®ƒæ€ä¹ˆæ²»" -> "ç”²æµæ€ä¹ˆæ²»")
        # ==================================================
        standalone_question = await self.rewrite_query_based_on_history(question, history_messages)
        print(f"ğŸ§ [ç­–ç•¥1] ç‹¬ç«‹é—®é¢˜: {standalone_question}")

        # ==================================================
        # 3. ã€ç­–ç•¥äºŒ & ä¸‰ï¼šå¤šè·¯æ‰©å±•ä¸åˆ†è§£ã€‘ (Expansion & Decomposition)
        # ç›®çš„ï¼šç”Ÿæˆå¤šä¸ªæœç´¢è§†è§’å’Œå­é—®é¢˜
        # ==================================================
        # è¿™ä¸€æ­¥ä¼šç”Ÿæˆä¸€ä¸ªåˆ—è¡¨ï¼Œä¾‹å¦‚ ["ç”²æµæ²»ç–—æ–¹æ¡ˆ", "å„¿ç«¥ç”²æµç”¨è¯", "ç”²æµå‘çƒ§æŠ¤ç†"]
        queries_to_search = await self.generate_multi_queries(standalone_question)
        print(f"ğŸš€ [ç­–ç•¥2&3] ç”Ÿæˆçš„æœç´¢è¯: {queries_to_search}")

        # ==================================================
        # 4. ã€å¹¶è¡Œå‘é‡æ£€ç´¢ & å»é‡ã€‘ (Retrieval & Deduplication)
        # ç›®çš„ï¼šæ‹¿ç€æ‰€æœ‰æœç´¢è¯å»åº“é‡Œæ‰¾ï¼Œå¹¶åˆå¹¶ç»“æœ
        # ==================================================
        all_docs = []

        # éå†æ‰€æœ‰ç”Ÿæˆçš„æŸ¥è¯¢è¯è¿›è¡Œæ£€ç´¢
        # æ³¨æ„ï¼švector_store.similarity_search æ˜¯åŒæ­¥çš„ï¼Œè¿™é‡Œç”¨å¾ªç¯
        for q in queries_to_search:
            # è¿™é‡Œçš„ k=2 å¯ä»¥å°ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬æœäº†å¾ˆå¤šæ¬¡ï¼Œæ€»é‡ä¼šå¾ˆå¤š
            docs = vector_store.similarity_search(q, k=2)
            all_docs.extend(docs)

        # ã€æ–‡æ¡£å»é‡ã€‘ï¼šæ ¹æ® page_content å»é‡ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡é‡å¤
        unique_docs = self._deduplicate_documents(all_docs)
        print(f"ğŸ“š [æœ€ç»ˆ] æ£€ç´¢åˆ° {len(unique_docs)} ä¸ªä¸é‡å¤ç‰‡æ®µ")

        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = "\n\n".join([doc.page_content for doc in unique_docs])
        response = await llm.ainvoke(
            [{'role': 'user', 'content': f'å¸®æˆ‘è¯„ä¼°ä¸€ä¸‹å¬å›ç‡:ç”¨æˆ·çš„é—®é¢˜{question},RAGæ£€ç´¢ç»“æœ:{context_text}'}])
        print(f'ğŸ¤– LLM RAGè¯„ä¼°å›å¤:{response.content}')
        # ==================================================
        # 5. ç”Ÿæˆæœ€ç»ˆå›ç­”
        # ==================================================
        rag_system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ã€å‚è€ƒä¿¡æ¯ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

        å›ç­”åŸåˆ™ï¼š
        1. ç»¼åˆå¤šæ¡å‚è€ƒä¿¡æ¯ï¼Œé€»è¾‘æ¸…æ™°åœ°å›ç­”ã€‚
        2. å¦‚æœå‚è€ƒä¿¡æ¯ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ï¼Œä¸è¦çç¼–ã€‚
        3. è¯­æ°”äº²åˆ‡ã€ä¸“ä¸šã€‚

        ã€å‚è€ƒä¿¡æ¯ã€‘:
        {context_text}
        """

        final_messages = [
            SystemMessage(content=rag_system_prompt),
            *history_messages,
            HumanMessage(content=question)  # ç»™ LLM çœ‹åŸå§‹é—®é¢˜ï¼Œä¿æŒå¯¹è¯æµç•…åº¦
        ]

        response_stream = llm.astream(final_messages)

        final_answer = ""
        async for chunk in response_stream:
            content = chunk.content
            if content:
                final_answer += content
                yield f'data: {content}\n\n'

        # ==================================================
        # 6. å­˜å…¥å†å²è®°å½•
        # ==================================================
        new_history = [
            {'role': 'user', 'content': question},
            {"role": "assistant", "content": final_answer}
        ]
        await redis_client.rpush(chat_message_list_key, *[json.dumps(m) for m in new_history])

        chat_message_hash_key = f'chat:message:hash:{user_id}:{session_id}'
        await redis_client.hset(chat_message_hash_key, mapping={
            "last_message": final_answer[:20]
        })

    # -------------------------------------------------------------------------
    # è¾…åŠ©æ–¹æ³•åŒºåŸŸ
    # -------------------------------------------------------------------------

    async def rewrite_query_based_on_history(self, question, history_messages) -> str:
        """
        ã€ç­–ç•¥ä¸€å®ç°ã€‘ï¼šåŸºäºå†å²è®°å½•é‡å†™é—®é¢˜
        """
        if not history_messages:
            return question

        prompt = """
        ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¼˜åŒ–ä¸“å®¶ã€‚
        è¯·æ ¹æ®ã€å¯¹è¯å†å²ã€‘å’Œã€ç”¨æˆ·çš„æœ€æ–°é—®é¢˜ã€‘ï¼Œå°†ç”¨æˆ·çš„é—®é¢˜é‡å†™ä¸ºä¸€ä¸ª**ç‹¬ç«‹çš„ã€è¯­ä¹‰å®Œæ•´çš„**å¥å­ã€‚
        ä¾‹å¦‚ï¼šå°†"å®ƒæœ‰ä»€ä¹ˆå‰¯ä½œç”¨"é‡å†™ä¸º"å¥¥å¸ä»–éŸ¦æœ‰ä»€ä¹ˆå‰¯ä½œç”¨"ã€‚
        **åªè¾“å‡ºé‡å†™åçš„å¥å­ï¼Œä¸è¦è§£é‡Šã€‚**
        """

        messages = [
            SystemMessage(content=prompt),
            HumanMessage(content=f"ã€å¯¹è¯å†å²ã€‘: {history_messages}\nã€ç”¨æˆ·æœ€æ–°çš„é—®é¢˜ã€‘: {question}")
        ]

        # ä½¿ç”¨ ainvoke å¼‚æ­¥è°ƒç”¨
        response = await llm.ainvoke(messages)
        return response.content.strip()

    async def generate_multi_queries(self, original_query: str) -> List[str]:
        """
        ã€ç­–ç•¥äºŒ & ä¸‰å®ç°ã€‘ï¼šå¤šè§’åº¦æ‰©å±• + é—®é¢˜åˆ†è§£
        """
        prompt = """
        ä½ æ˜¯ä¸€ä¸ªAIæœç´¢åŠ©æ‰‹ã€‚ä¸ºäº†æ›´ç²¾å‡†åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œè¯·åŸºäºåŸå§‹é—®é¢˜ç”Ÿæˆ 3 ä¸ªä¸åŒçš„æœç´¢æŸ¥è¯¢è¯ã€‚

        ç”Ÿæˆè§„åˆ™ï¼š
        1. **åŒä¹‰æ‰©å±•**ï¼šåŒ…å«ç›¸å…³çš„åŒ»å­¦æœ¯è¯­æˆ–åˆ«åï¼ˆå¦‚"å‘çƒ§"->"å‘çƒ­å¤„ç†"ï¼‰ã€‚
        2. **é—®é¢˜æ‹†è§£**ï¼šå¦‚æœé—®é¢˜å¤æ‚ï¼Œæ‹†è§£ä¸ºå­é—®é¢˜ï¼ˆå¦‚"ç”²æµä¹™æµåŒºåˆ«"->"ç”²æµç—‡çŠ¶"å’Œ"ä¹™æµç—‡çŠ¶"ï¼‰ã€‚
        3. **ä¿ç•™åŸæ„**ï¼šå¿…é¡»åŒ…å«åŸå§‹é—®é¢˜çš„æ ¸å¿ƒæŸ¥è¯¢ã€‚

        è¯·ç›´æ¥è¾“å‡º 3 è¡ŒæŸ¥è¯¢è¯ï¼Œæ¯è¡Œä¸€ä¸ªï¼Œä¸è¦å¸¦åºå·æˆ–è§£é‡Šã€‚

        åŸå§‹é—®é¢˜: {question}
        
        ç¦æ­¢ï¼š
            ç¦æ­¢ä¿®æ”¹ç”¨æˆ·åŸæœ¬çš„æ„æ€ï¼Œæ¯”å¦‚ï¼šæˆ‘å˜´é‡Œé¢æœ‰ç‚¹ç–¼ï¼Œæ€ä¹ˆå›äº‹ï¼Ÿç¦æ­¢ä¿®æ”¹æˆï¼šç‰™ç–¼éƒ½æœ‰å“ªäº›ç—‡çŠ¶ï¼Ÿ
        """

        messages = [SystemMessage(content=prompt.format(question=original_query))]

        response = await llm.ainvoke(messages)
        content = response.content.strip()

        # è§£æç»“æœï¼ŒæŒ‰è¡Œåˆ†å‰²
        queries = [q.strip() for q in content.split('\n') if q.strip()]

        # å…œåº•ï¼šå¦‚æœç”Ÿæˆå¤±è´¥ï¼Œè‡³å°‘ä¿ç•™åŸé—®é¢˜
        if not queries:
            return [original_query]

        # æŠŠåŸå§‹é—®é¢˜ä¹ŸåŠ è¿›å»ï¼Œç¡®ä¿ä¸‡æ— ä¸€å¤±
        if original_query not in queries:
            queries.insert(0, original_query)

        return queries[:4]  # é™åˆ¶æœ€å¤šæœ 4 æ¬¡ï¼Œé˜²æ­¢å¤ªæ…¢

    def _deduplicate_documents(self, documents: List[Document]) -> List[Document]:
        """
        æ–‡æ¡£å»é‡å·¥å…·ï¼šæ ¹æ® page_content å»é‡
        """
        unique_docs = []
        seen_content = set()

        for doc in documents:
            # å–å†…å®¹çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºæŒ‡çº¹ï¼Œæˆ–è€…ç›´æ¥ç”¨æ•´ä¸ªcontent
            content_fingerprint = doc.page_content.strip()

            if content_fingerprint not in seen_content:
                seen_content.add(content_fingerprint)
                unique_docs.append(doc)

        return unique_docs


ai_service = AiService()
