import csv
import os
import time
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup


class MedliveSpider:
    def __init__(self, cookie_str):
        self.session = requests.Session()
        self.base_url = 'https://drugs.medlive.cn'
        self.index_url = "https://drugs.medlive.cn/v2/drugref/drugTree/index"

        # --- æ–°å¢ï¼šå®šä¹‰è®°å½•è¿›åº¦çš„æ–‡ä»¶è·¯å¾„ ---
        self.history_file = "crawled_urls.txt"
        # --- æ–°å¢ï¼šå¯åŠ¨æ—¶åŠ è½½å·²çˆ¬å–çš„ URL é›†åˆ ---
        self.crawled_set = self._load_history()

        # Cookie å’Œ Header è®¾ç½®ä¿æŒä¸å˜
        cookies = {}
        for item in cookie_str.split(';'):
            item = item.strip()
            if '=' in item:
                k, v = item.split('=', 1)
                cookies[k] = v

        self.session.cookies.update(cookies)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://drugs.medlive.cn/'
        })

        self.one_drug_data = []
        self.two_drug_data = []
        self.three_drug_data = []
        self.detail_data = []

    # ==========================================
    # æ ¸å¿ƒä¿®æ”¹ 1: åŠ è½½å†å²è®°å½•
    # ==========================================
    def _load_history(self):
        """å¯åŠ¨æ—¶è¯»å– txt æ–‡ä»¶ï¼Œè¿”å›å·²çˆ¬å–çš„ URL é›†åˆ"""
        crawled = set()
        if os.path.exists(self.history_file):
            with open(self.history_file, 'r', encoding='utf-8') as f:
                for line in f:
                    crawled.add(line.strip())
            print(f"ğŸ“– [æ–­ç‚¹ç»­ä¼ ] å·²åŠ è½½ {len(crawled)} æ¡å†å²è®°å½•ï¼Œå°†è·³è¿‡è¿™äº›é¡µé¢ã€‚")
        else:
            print("ğŸ“– [æ–­ç‚¹ç»­ä¼ ] æœªå‘ç°å†å²è®°å½•ï¼Œå°†ä»å¤´å¼€å§‹ã€‚")
        return crawled

    # ==========================================
    # æ ¸å¿ƒä¿®æ”¹ 2: è®°å½•å†å²è®°å½•
    # ==========================================
    def _record_history(self, url_list):
        """å°†æˆåŠŸä¿å­˜çš„ URL å†™å…¥ txt æ–‡ä»¶"""
        try:
            with open(self.history_file, 'a', encoding='utf-8') as f:
                for url in url_list:
                    f.write(url + '\n')
                    self.crawled_set.add(url)  # åŒæ—¶æ›´æ–°å†…å­˜ä¸­çš„é›†åˆ
        except Exception as e:
            print(f"âš ï¸ è®°å½•è¿›åº¦å¤±è´¥: {e}")

    def save_to_csv(self, data_batch, filename="medlive_drugs.csv"):
        """åˆ†æ‰¹å°†æ•°æ®å†™å…¥ CSV æ–‡ä»¶ï¼Œå¹¶è®°å½•è¿›åº¦"""
        if not data_batch:
            return

        headers = [
            'å¤§ç±»', 'è¯ç‰©ç±»åˆ«', 'é€šç”¨å', 'æ¥æºé“¾æ¥',
            'æˆåˆ†', 'æ€§çŠ¶', 'é€‚åº”ç—‡', 'è§„æ ¼', 'ç”¨æ³•ç”¨é‡',
            'ä¸è‰¯ååº”', 'ç¦å¿Œ', 'æ³¨æ„äº‹é¡¹',
            'å­•å¦‡åŠå“ºä¹³æœŸå¦‡å¥³ç”¨è¯', 'å„¿ç«¥ç”¨è¯', 'è€å¹´ç”¨è¯',
            'è¯ç‰©ç›¸äº’ä½œç”¨', 'è¯ç‰©è¿‡é‡', 'è¯ç†æ¯’ç†', 'è¯ä»£åŠ¨åŠ›å­¦',
            'è´®è—', 'åŒ…è£…', 'æœ‰æ•ˆæœŸ', 'æ‰§è¡Œæ ‡å‡†', 'æ‰¹å‡†æ–‡å·', 'ç”Ÿäº§ä¼ä¸š'
        ]

        file_exists = os.path.isfile(filename)

        try:
            with open(filename, mode='a', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=headers, extrasaction='ignore')
                if not file_exists:
                    writer.writeheader()
                writer.writerows(data_batch)
                print(f"   ğŸ’¾ [ç³»ç»Ÿ] å·²è‡ªåŠ¨ä¿å­˜ {len(data_batch)} æ¡æ•°æ®åˆ° CSV")

            # ==========================================
            # æ ¸å¿ƒä¿®æ”¹ 3: CSV å†™å…¥æˆåŠŸåï¼Œç«‹å³è®°å½•è¿›åº¦
            # ==========================================
            saved_urls = [item['æ¥æºé“¾æ¥'] for item in data_batch]
            self._record_history(saved_urls)

        except Exception as e:
            print(f"âŒ ä¿å­˜ CSV å‡ºé”™: {e}")

    def get_directory(self):
        """æ­¥éª¤ä¸€ï¼šè·å–ç›®å½•é¡µ"""
        # ... (ä¿æŒåŸä»£ç ä¸å˜) ...
        print(f"[-] æ­£åœ¨è·å–ç›®å½•: {self.index_url}")
        try:
            resp = self.session.get(self.index_url)
            if resp.status_code != 200: return
            soup = BeautifulSoup(resp.text, 'html.parser')
            drug_titles = soup.find_all('div', class_='drug_title')
            for title in drug_titles:
                cate_name = title.get_text(strip=True)
                table = title.find_next_sibling('table')
                if not table: continue
                a_tags = table.find_all('a', href=True)
                for a in a_tags:
                    drug_name = a.get_text(strip=True)
                    href = a['href'].strip()
                    if not drug_name or href == '#': continue
                    self.one_drug_data.append({
                        'å¤§ç±»': cate_name,
                        'è¯ç‰©ç±»åˆ«': drug_name,
                        'é“¾æ¥': urljoin(self.base_url, href)
                    })
            print(f"[-] å·²è§£æ {len(self.one_drug_data)} æ¡åˆ†ç±»æ•°æ®")
        except Exception as e:
            print(f"âŒ è§£æç›®å½•å‡ºé”™: {e}")

    def get_two_directory(self):
        """æ­¥éª¤äºŒï¼šè®¿é—®äºŒçº§åˆ†ç±»"""
        # ... (ä¿æŒåŸä»£ç ä¸å˜) ...
        if not self.one_drug_data: return
        print("\n[-] å¼€å§‹è·å–äºŒçº§åˆ—è¡¨...")
        for item in self.one_drug_data:  # å…¨é‡è·‘
            target_url = item['é“¾æ¥']
            try:
                time.sleep(5)
                resp = self.session.get(target_url)
                soup = BeautifulSoup(resp.text, 'html.parser')
                drug_titles = soup.find_all('div', class_='drug_title')
                for title in drug_titles:
                    cate_name = title.get_text(strip=True)
                    table = title.find_next_sibling('div', class_='drug_list')
                    if not table: continue
                    a_tags = table.find_all('a', href=True)
                    for a in a_tags:
                        self.two_drug_data.append({
                            'å¤§ç±»': cate_name,
                            'è¯ç‰©ç±»åˆ«': a.get_text(strip=True),
                            'é“¾æ¥': a['href'].strip(),
                            'æ ‡é¢˜': a.get('title')
                        })
            except Exception as e:
                print(f"âŒ è¯·æ±‚å‡ºé”™: {e}")

    def get_three_directory(self):
        """æ­¥éª¤ä¸‰ï¼šè®¿é—®ä¸‰çº§åˆ†ç±»"""
        # ... (ä¿æŒåŸä»£ç ä¸å˜) ...
        if not self.two_drug_data: return
        print("\n[-] å¼€å§‹è·å–ä¸‰çº§åˆ—è¡¨...")
        for item in self.two_drug_data:
            target_url = urljoin(self.base_url, item['é“¾æ¥'])
            try:
                time.sleep(5)
                resp = self.session.get(target_url)
                soup = BeautifulSoup(resp.text, 'html.parser')
                box_list = soup.find_all('div', class_='box1')
                for box in box_list:
                    sub_box = box.find('div', class_='medince-name')
                    if sub_box and sub_box.find('a'):
                        a = sub_box.find('a')
                        self.three_drug_data.append({
                            'å¤§ç±»': item.get('å¤§ç±»'),
                            'è¯ç‰©ç±»åˆ«': item.get('è¯ç‰©ç±»åˆ«'),
                            'é“¾æ¥': urljoin(self.base_url, a['href']),
                            'æ ‡é¢˜': a.get_text(strip=True)
                        })
            except Exception as e:
                print(f"âŒ è®¿é—®å‡ºé”™: {e}")
        print(f"[-] ä¸‰çº§åˆ—è¡¨è·å–å®Œæˆï¼Œå…± {len(self.three_drug_data)} æ¡")

    def get_detail(self):
        """æ­¥éª¤å››ï¼šé€šç”¨è¯¦æƒ…é¡µè§£æï¼ˆå«æ–­ç‚¹ç»­ä¼ ï¼‰"""
        if not self.three_drug_data:
            print("[-] æ²¡æœ‰æ•°æ®å¯æŠ“å–")
            return

        total_count = len(self.three_drug_data)
        print(f"\n[-] å‡†å¤‡æŠ“å–è¯¦æƒ…é¡µï¼Œä»»åŠ¡é˜Ÿåˆ—æ€»æ•°: {total_count}")

        batch_buffer = []
        BATCH_SIZE = 5

        # ç»Ÿè®¡è·³è¿‡æ•°é‡
        skip_count = 0

        for i, item in enumerate(self.three_drug_data):
            target_url = item['é“¾æ¥']

            # ==========================================
            # æ ¸å¿ƒä¿®æ”¹ 4: æ£€æŸ¥æ˜¯å¦å·²çˆ¬å–
            # ==========================================
            if target_url in self.crawled_set:
                skip_count += 1
                # æ¯è·³è¿‡ 100 ä¸ªæ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼Œé¿å…åˆ·å±
                if skip_count % 100 == 0:
                    print(f"â© å·²è·³è¿‡ {skip_count} æ¡å·²å­˜åœ¨çš„è®°å½•...")
                continue

            # æ‰“å°å½“å‰è¿›åº¦
            print(f"[-] [{i + 1}/{total_count}] æ­£åœ¨è¯·æ±‚: {item['æ ‡é¢˜']}")

            try:
                time.sleep(5)
                resp = self.session.get(target_url)

                if "auth/login" in resp.url or "ä¼šå‘˜ç™»å½•" in resp.text:
                    print(f"âŒ å¤±è´¥: Cookie å¤±æ•ˆ")
                    self.save_to_csv(batch_buffer)  # é€€å‡ºå‰ä¿å­˜å·²æœ‰æ•°æ®
                    break

                soup = BeautifulSoup(resp.text, 'html.parser')

                one_drug_record = {
                    'å¤§ç±»': item.get('å¤§ç±»', ''),
                    'è¯ç‰©ç±»åˆ«': item.get('è¯ç‰©ç±»åˆ«', ''),
                    'é€šç”¨å': item.get('æ ‡é¢˜', ''),
                    'æ¥æºé“¾æ¥': target_url
                }

                title_divs = soup.find_all('div', class_='inner_title clearfix')
                for title_div in title_divs:
                    key = title_div.get_text(strip=True)
                    content_parts = []
                    curr = title_div.next_sibling
                    while curr:
                        if curr.name == 'div' and 'inner_title' in curr.get('class', []):
                            break
                        if curr.name:
                            text = curr.get_text(separator='\n', strip=True)
                            if text: content_parts.append(text)
                        curr = curr.next_sibling

                    full_content = "\n".join(content_parts)
                    if full_content:
                        one_drug_record[key] = full_content

                batch_buffer.append(one_drug_record)
                print(f"   âœ… è§£ææˆåŠŸ")

                if len(batch_buffer) >= BATCH_SIZE:
                    # save_to_csv å†…éƒ¨ä¼šè‡ªåŠ¨è®°å½•è¿™äº› URL åˆ° crawled_urls.txt
                    self.save_to_csv(batch_buffer)
                    batch_buffer = []

            except Exception as e:
                print(f"âŒ è®¿é—®å‡ºé”™: {e}")

        # æœ€åä¿å­˜å‰©ä½™çš„æ•°æ®
        if batch_buffer:
            self.save_to_csv(batch_buffer)

        print(f"\n[+] æ‰€æœ‰å·¥ä½œå®Œæˆï¼å…±è·³è¿‡ {skip_count} æ¡å†å²æ•°æ®ã€‚")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # è¯·å¡«å…¥æœ€æ–°çš„ Cookie
    MY_COOKIE = 'ymt_pk_id=e58e7dbb8311834e; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2219b43bbe9cf9a6-0f28744e6157dc8-26061a51-2073600-19b43bbe9d012a2%22%2C%22first_id%22%3A%22%22%2C%22props%22%3A%7B%7D%2C%22identities%22%3A%22eyIkaWRlbnRpdHlfY29va2llX2lkIjoiMTliNDNiYmU5Y2Y5YTYtMGYyODc0NGU2MTU3ZGM4LTI2MDYxYTUxLTIwNzM2MDAtMTliNDNiYmU5ZDAxMmEyIn0%3D%22%2C%22history_login_id%22%3A%7B%22name%22%3A%22%22%2C%22value%22%3A%22%22%7D%2C%22%24device_id%22%3A%2219b43bbe9cf9a6-0f28744e6157dc8-26061a51-2073600-19b43bbe9d012a2%22%7D; Hm_lvt_62d92d99f7c1e7a31a11759de376479f=1766651114,1766658742; ymtinfo=eyJ1aWQiOiI2ODk4NDUwIiwicmVzb3VyY2UiOiIiLCJleHRfdmVyc2lvbiI6IjEiLCJhcHBfbmFtZSI6IiJ9; _pk_ref.3.a971=%5B%22%22%2C%22%22%2C1766716943%2C%22https%3A%2F%2Fwww.google.com%2F%22%5D; _pk_ses.3.a971=*; JSESSIONID=8296E6EFB4F50E7FB063166CEA85D8C3; _pk_id.3.a971=e58e7dbb8311834e.1766318358.9.1766716975.1766667254.'

    bot = MedliveSpider(MY_COOKIE)

    # çˆ¬å–æµç¨‹
    # æ³¨æ„ï¼šå‰ä¸‰ä¸ªæ­¥éª¤è¿˜æ˜¯éœ€è¦è¿è¡Œçš„ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦ç”Ÿæˆä»»åŠ¡åˆ—è¡¨
    # ä½†æ˜¯å› ä¸ºæœ‰äº†æ–­ç‚¹ç»­ä¼ ï¼Œå³ä½¿ä»»åŠ¡åˆ—è¡¨ç”Ÿæˆäº†ï¼Œget_detail ä¹Ÿä¼šé£å¿«åœ°è·³è¿‡å·²ç»åšè¿‡çš„
    bot.get_directory()
    bot.get_two_directory()
    bot.get_three_directory()

    # è¿™é‡Œå¼€å§‹æ‰æ˜¯çœŸæ­£çš„è€—æ—¶æ“ä½œï¼Œä¼šæ”¯æŒæ–­ç‚¹ç»­ä¼ 
    bot.get_detail()